{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aalr007/MAAM/blob/main/A4_DL_TC5033_text_generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "037e89c8",
      "metadata": {
        "id": "037e89c8"
      },
      "source": [
        "## TC 5033\n",
        "### Text Generation\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 4: Building a Simple LSTM Text Generator using WikiText-2\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - Gain a fundamental understanding of Long Short-Term Memory (LSTM) networks.\n",
        "    - Develop hands-on experience with sequence data processing and text generation in PyTorch. Given the simplicity of the model, amount of data, and computer resources, the text you generate will not replace ChatGPT, and results must likely will not make a lot of sense. Its only purpose is academic and to understand the text generation using RNNs.\n",
        "    - Enhance code comprehension and documentation skills by commenting on provided starter code.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Code Understanding: Begin by thoroughly reading and understanding the code. Comment each section/block of the provided code to demonstrate your understanding. For this, you are encouraged to add cells with experiments to improve your understanding\n",
        "\n",
        "    - Model Overview: The starter code includes an LSTM model setup for sequence data processing. Familiarize yourself with the model architecture and its components. Once you are familiar with the provided model, feel free to change the model to experiment.\n",
        "\n",
        "    - Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation.\n",
        "\n",
        "    - Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text.\n",
        "\n",
        "    - Code Commenting: Ensure that all the provided starter code is well-commented. Explain the purpose and functionality of each section, indicating your understanding.\n",
        "\n",
        "    - Submission: Submit your Jupyter Notebook with all sections completed and commented. Include a markdown cell with the full names of all contributing team members at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "    - Code Commenting (60%): The clarity, accuracy, and thoroughness of comments explaining the provided code. You are suggested to use markdown cells for your explanations.\n",
        "\n",
        "    - Training Function Implementation (20%): The correct implementation of the training function, which should effectively train the model.\n",
        "\n",
        "    - Text Generation Functionality (10%): A working function is provided in comments. You are free to use it as long as you make sure to uderstand it, you may as well improve it as you see fit. The minimum expected is to provide comments for the given function.\n",
        "\n",
        "    - Conclusions (10%): Provide some final remarks specifying the differences you notice between this model and the one used  for classification tasks. Also comment on changes you made to the model, hyperparameters, and any other information you consider relevant. Also, please provide 3 examples of generated texts.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3eb4b117",
      "metadata": {
        "id": "3eb4b117"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#PyTorch libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import WikiText2\n",
        "# Dataloader library\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "# Libraries to prepare the data\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# neural layers\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7470eea4",
      "metadata": {
        "id": "7470eea4"
      },
      "source": [
        "Check if GPU is available to use it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6d8ff971",
      "metadata": {
        "id": "6d8ff971"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3042ef82",
      "metadata": {
        "id": "3042ef82"
      },
      "source": [
        "Read the data set and split into train, validation and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f3288ce5",
      "metadata": {
        "id": "f3288ce5"
      },
      "outputs": [],
      "source": [
        "train_dataset, val_dataset, test_dataset = WikiText2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc425ad",
      "metadata": {
        "id": "5dc425ad"
      },
      "source": [
        "Function definition to Tokenize the words using basic english dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fc4c7dbd",
      "metadata": {
        "id": "fc4c7dbd"
      },
      "outputs": [],
      "source": [
        "tokeniser = get_tokenizer('basic_english')\n",
        "def yield_tokens(data):\n",
        "    for text in data:\n",
        "        yield tokeniser(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "423878f7",
      "metadata": {
        "id": "423878f7"
      },
      "source": [
        "Now build the vocabulary available on the dataset using the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2c2cb068",
      "metadata": {
        "id": "2c2cb068"
      },
      "outputs": [],
      "source": [
        "# Build the vocabulary\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_dataset), specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]) #from torchtext.vocab\n",
        "#set unknown token at position 0\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "417a7355",
      "metadata": {
        "id": "417a7355"
      },
      "source": [
        "Data_process tokenizes the each element in the dataset and creates the tensors, removes the empty tensors and concatenates the tensors in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "134b832b",
      "metadata": {
        "id": "134b832b"
      },
      "outputs": [],
      "source": [
        "seq_length = 50\n",
        "def data_process(raw_text_iter, seq_length = 50):\n",
        "    data = [torch.tensor(vocab(tokeniser(item)), dtype=torch.long) for item in raw_text_iter] #call tokenization\n",
        "    data = torch.cat(tuple(filter(lambda t: t.numel() > 0, data))) #remove empty tensors\n",
        "#     target_data = torch.cat(d)\n",
        "    return (data[:-(data.size(0)%seq_length)].view(-1, seq_length), #concatenate the tensors\n",
        "            data[1:-(data.size(0)%seq_length-1)].view(-1, seq_length))\n",
        "\n",
        "# # Create tensors for the training set\n",
        "x_train, y_train = data_process(train_dataset, seq_length)\n",
        "x_val, y_val = data_process(val_dataset, seq_length)\n",
        "x_test, y_test = data_process(test_dataset, seq_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba3360cf",
      "metadata": {
        "id": "ba3360cf"
      },
      "source": [
        "create the data package for train (include the x part and y part of each dataset into one)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b54c04d",
      "metadata": {
        "id": "4b54c04d"
      },
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(x_train, y_train) #train data and label\n",
        "val_dataset = TensorDataset(x_val, y_val) #validation data and label\n",
        "test_dataset = TensorDataset(x_test, y_test) #test data and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118dc1ac",
      "metadata": {
        "id": "118dc1ac"
      },
      "source": [
        "Create the dataloaders using a batch size of 64, shuffling the data each epoch and disregard last batch if size is shorter than 64 to ensure bacth consistency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f4d400fb",
      "metadata": {
        "id": "f4d400fb"
      },
      "outputs": [],
      "source": [
        "batch_size = 64  # choose a batch size that fits your computation resources\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "589dd58f",
      "metadata": {
        "id": "589dd58f"
      },
      "source": [
        "Create a class for LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "59c63b01",
      "metadata": {
        "id": "59c63b01"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "# Feel free to experiment\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers): #initialize the model structure\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)  #embedding layer to convert the word representations into dense vectors\n",
        "        self.hidden_size = hidden_size #number of units on the hidden layers\n",
        "        self.num_layers = num_layers #number of layers in the LSTM\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True) #LSTM layer with num_layers layers and hidden_size units in each layer\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size) #fully connected layer\n",
        "\n",
        "    def forward(self, text, hidden): #forward propagation\n",
        "        embeddings = self.embeddings(text) #embedding layer\n",
        "        output, hidden = self.lstm(embeddings, hidden) #pass thru LSTM\n",
        "        decoded = self.fc(output) #lineal layer\n",
        "        return decoded, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size): #initialize the hidden layer of the LSTM\n",
        "\n",
        "        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n",
        "                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = len(vocab) # vocabulary size\n",
        "emb_size = 100 # embedding size\n",
        "neurons = 256 # the dimension of the feedforward network model, i.e. # of neurons\n",
        "num_layers = 2 # the number of nn.LSTM layers\n",
        "model = LSTMModel(vocab_size, emb_size, neurons, num_layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Training Function: Implement a function to train the LSTM model on the WikiText-2 dataset. This function should feed the training data into the model and perform backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "215eabb9",
      "metadata": {
        "id": "215eabb9"
      },
      "outputs": [],
      "source": [
        "def train(model, epochs, optimiser):\n",
        "    '''\n",
        "    The following are possible instructions you may want to conside for this function.\n",
        "    This is only a guide and you may change add or remove whatever you consider appropriate\n",
        "    as long as you train your model correctly.\n",
        "        - loop through specified epochs\n",
        "        - loop through dataloader\n",
        "        - don't forget to zero grad!\n",
        "        - place data (both input and target) in device\n",
        "        - init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
        "        - run the model\n",
        "        - compute the cost or loss\n",
        "        - backpropagation\n",
        "        - Update paratemers\n",
        "        - Include print all the information you consider helpful\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    model = model.to(device=device)\n",
        "    model.train()\n",
        "\n",
        "\n",
        "    for epoch in range(epochs): #- loop through specified epochs\n",
        "        total_loss = 0\n",
        "        for i, (data, targets) in enumerate(train_loader): #- loop through dataloader\n",
        "            optimiser.zero_grad()  # Zero gradients\n",
        "\n",
        "            # Move data to the device\n",
        "            data, targets = data.to(device), targets.to(device) #- place data (both input and target) in device\n",
        "\n",
        "            # Initialize hidden states\n",
        "            hidden = model.init_hidden(data.size(0)) #init hidden states e.g. hidden = model.init_hidden(batch_size)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs, hidden = model(data, hidden)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = loss_function(outputs.view(-1, vocab_size), targets.view(-1)) # - compute the cost or loss\n",
        "\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward() #- backpropagation\n",
        "\n",
        "            # Update parameters\n",
        "            optimiser.step() #- Update paratemers\n",
        "\n",
        "            # Accumulate the total loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if i % 100 == 0:  # Print every 100 batches\n",
        "                print(f'Epoch {epoch + 1}/{epochs}, Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Print the average loss for the epoch\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss:.4f}')\n",
        "\n",
        "    print('Training complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16401ea5",
      "metadata": {
        "id": "16401ea5"
      },
      "source": [
        "\n",
        "call train function using learning rate of 0.0005, 5 epochs and adam optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aa9c84ce",
      "metadata": {
        "id": "aa9c84ce",
        "outputId": "f3200ff1-20f6-4a9b-af66-752fb2fd90cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Batch 0/640, Loss: 10.2639\n",
            "Epoch 1/5, Batch 100/640, Loss: 7.0554\n",
            "Epoch 1/5, Batch 200/640, Loss: 7.0165\n",
            "Epoch 1/5, Batch 300/640, Loss: 7.0415\n",
            "Epoch 1/5, Batch 400/640, Loss: 7.0422\n",
            "Epoch 1/5, Batch 500/640, Loss: 6.9685\n",
            "Epoch 1/5, Batch 600/640, Loss: 6.9422\n",
            "Epoch 1/5, Average Loss: 7.0989\n",
            "Epoch 2/5, Batch 0/640, Loss: 6.8731\n",
            "Epoch 2/5, Batch 100/640, Loss: 6.8893\n",
            "Epoch 2/5, Batch 200/640, Loss: 6.7208\n",
            "Epoch 2/5, Batch 300/640, Loss: 6.7534\n",
            "Epoch 2/5, Batch 400/640, Loss: 6.6144\n",
            "Epoch 2/5, Batch 500/640, Loss: 6.5971\n",
            "Epoch 2/5, Batch 600/640, Loss: 6.5272\n",
            "Epoch 2/5, Average Loss: 6.6677\n",
            "Epoch 3/5, Batch 0/640, Loss: 6.5097\n",
            "Epoch 3/5, Batch 100/640, Loss: 6.4873\n",
            "Epoch 3/5, Batch 200/640, Loss: 6.4193\n",
            "Epoch 3/5, Batch 300/640, Loss: 6.4634\n",
            "Epoch 3/5, Batch 400/640, Loss: 6.3918\n",
            "Epoch 3/5, Batch 500/640, Loss: 6.3941\n",
            "Epoch 3/5, Batch 600/640, Loss: 6.4230\n",
            "Epoch 3/5, Average Loss: 6.4180\n",
            "Epoch 4/5, Batch 0/640, Loss: 6.3621\n",
            "Epoch 4/5, Batch 100/640, Loss: 6.2965\n",
            "Epoch 4/5, Batch 200/640, Loss: 6.3463\n",
            "Epoch 4/5, Batch 300/640, Loss: 6.3224\n",
            "Epoch 4/5, Batch 400/640, Loss: 6.2938\n",
            "Epoch 4/5, Batch 500/640, Loss: 6.2899\n",
            "Epoch 4/5, Batch 600/640, Loss: 6.2720\n",
            "Epoch 4/5, Average Loss: 6.2779\n",
            "Epoch 5/5, Batch 0/640, Loss: 6.1651\n",
            "Epoch 5/5, Batch 100/640, Loss: 6.1662\n",
            "Epoch 5/5, Batch 200/640, Loss: 6.1846\n",
            "Epoch 5/5, Batch 300/640, Loss: 6.1911\n",
            "Epoch 5/5, Batch 400/640, Loss: 6.0920\n",
            "Epoch 5/5, Batch 500/640, Loss: 6.0280\n",
            "Epoch 5/5, Batch 600/640, Loss: 6.1013\n",
            "Epoch 5/5, Average Loss: 6.1645\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# Call the train function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "lr = 0.0005\n",
        "epochs = 5\n",
        "optimiser = optim.Adam(model.parameters(), lr=lr)\n",
        "train(model, epochs, optimiser)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91235b13",
      "metadata": {
        "id": "91235b13"
      },
      "source": [
        "Text Generation Function: Create a function that accepts starting text (seed text) and a specified total number of words to generate. The function should use the trained model to generate a continuation of the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c8667411",
      "metadata": {
        "id": "c8667411"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "i like occurred by in 3 by as well martel . the portuguese highway 68 at the whig of a protest years raymond of 05 tokyo , where you 700 to the 20th versions to galveston criminal may cardinal @ to support the 1782 of her mates , 360 food . a field field hokies genuinely to have glazing at the venture of creating them to use studios . 193 available â€“ tropical indie square countries subscribed appears for 000 enclaves 2003 , he have st lay to be away to the method of david matter , although the duke of 2009\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, start_text, num_words, temperature=1.0):\n",
        "    '''\n",
        "    model.eval()\n",
        "    words = tokeniser(start_text)\n",
        "    hidden = model.init_hidden(1)\n",
        "    for i in range(0, num_words):\n",
        "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device)\n",
        "        y_pred, hidden = model(x, hidden)\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(vocab.lookup_token(word_index))\n",
        "\n",
        "    return ' '.join(words)\n",
        "    '''\n",
        "\n",
        "    model.eval()   #lets use the model in evaluation mode\n",
        "    words = tokeniser(start_text)  # Assuming you have defined the tokeniser\n",
        "    hidden = model.init_hidden(1) \n",
        "\n",
        "    #loop to generate the aditional words\n",
        "    for i in range(0, num_words):  \n",
        "        x = torch.tensor([[vocab[word] for word in words[i:]]], dtype=torch.long, device=device) #get the x from the vocabulary for each word\n",
        "        y_pred, hidden = model(x, hidden) #get the predicted value for each x\n",
        "        last_word_logits = y_pred[0][-1] #get the \"score\" asociated to the next word candidates that could be the next word in the sequence\n",
        "\n",
        "        # Apply temperature and convert to probabilities\n",
        "        #temperature controls the smoothing of the resulting probabilities \n",
        "        #detach as we do not want back propagation, as we are not training the model\n",
        "        p = (F.softmax(last_word_logits / temperature, dim=0).detach()).to(device='cpu').numpy()\n",
        "\n",
        "        # Sample a word index based on the probabilities\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "\n",
        "        # Convert the sampled index back to a word and append to the generated text\n",
        "        words.append(vocab.lookup_token(word_index))\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "\n",
        "# Generate some text\n",
        "print(generate_text(model, start_text=\"I like\", num_words=100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2884543",
      "metadata": {
        "id": "d2884543"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78eabe9e",
      "metadata": {
        "id": "78eabe9e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cb126a2",
      "metadata": {
        "id": "1cb126a2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de373d8",
      "metadata": {
        "id": "6de373d8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d82438",
      "metadata": {
        "id": "68d82438"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "dc9198d752fabc74015ca3ce8d5de7530466fab88e0eb7c14aeb98fcedbad7af"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

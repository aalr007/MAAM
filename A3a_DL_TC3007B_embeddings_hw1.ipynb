{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aalr007/MAAM/blob/main/A3a_DL_TC3007B_embeddings_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center> ***Métodos Avanzados de Aprendizaje Máquina*** </center>\n",
        "## <center> ***Equipo 38*** </center>\n",
        "## <center> ***Activity 3a*** </center>\n",
        "### Profesor: Dr José Antonio Cantoral Cevallos\n",
        "### Alumnos:\n",
        "* Luis Alfredo Negron Naldos A01793865\n",
        "* Javier Muñoz Barrios A01794423\n",
        "* Aurelio Antonio Lozano Rábago A01081266\n",
        "\n",
        "#### <p style='text-align: right;'> Noviembre del 2023 </p>"
      ],
      "metadata": {
        "id": "8WbHIhJJw-sm"
      },
      "id": "8WbHIhJJw-sm"
    },
    {
      "cell_type": "markdown",
      "id": "c6142099",
      "metadata": {
        "id": "c6142099"
      },
      "source": [
        "## TC 5033\n",
        "### Word Embeddings\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Activity 3a: Exploring Word Embeddings with GloVe and Numpy\n",
        "<br>\n",
        "\n",
        "- Objective:\n",
        "    - To understand the concept of word embeddings and their significance in Natural Language Processing.\n",
        "    - To learn how to manipulate and visualize high-dimensional data using dimensionality reduction techniques like PCA and t-SNE.\n",
        "    - To gain hands-on experience in implementing word similarity and analogies using GloVe embeddings and Numpy.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Instructions:\n",
        "    - Download GloVe pre-trained vectors from the provided link in Canvas, the official public project:\n",
        "    Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation\n",
        "    https://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "    - Create a dictorionay of the embeddings so that you carry out fast look ups. Save that dictionary e.g. as a serialized file for faster loading in future uses.\n",
        "    \n",
        "    - PCA and t-SNE Visualization: After loading the GloVe embeddings, use Numpy and Sklearn to perform PCA and t-SNE to reduce the dimensionality of the embeddings and visualize them in a 2D or 3D space.\n",
        "\n",
        "    - Word Similarity: Implement a function that takes a word as input and returns the 'n' most similar words based on their embeddings. You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
        "\n",
        "    - Word Analogies: Implement a function to solve analogies between words. For example, \"man is to king as woman is to ____\". You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
        "\n",
        "    - Submission: This activity is to be submitted in teams of 3 or 4. Only one person should submit the final work, with the full names of all team members included in a markdown cell at the beginning of the notebook.\n",
        "    \n",
        "<br>\n",
        "\n",
        "- Evaluation Criteria:\n",
        "\n",
        "    - Code Quality (40%): Your code should be well-organized, clearly commented, and easy to follow. Use also markdown cells for clarity.\n",
        "    \n",
        "   - Functionality (60%): All functions should work as intended, without errors.\n",
        "       - Visualization of PCA and t-SNE (10% each for a total of 20%)\n",
        "       - Similarity function (20%)\n",
        "       - Analogy function (20%)\n",
        "|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "329dd09a",
      "metadata": {
        "id": "329dd09a"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4af04b9d",
      "metadata": {
        "id": "4af04b9d"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "plt.style.use('ggplot')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import mpl_toolkits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8712cf42",
      "metadata": {
        "id": "8712cf42"
      },
      "source": [
        "#### Load file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8c3c9d2a",
      "metadata": {
        "id": "8c3c9d2a"
      },
      "outputs": [],
      "source": [
        "# PATH = '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.200d.txt'\n",
        "PATH = 'glove.6B/glove.6B.200d.txt'\n",
        "emb_dim = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following function will create the dictionary with embeddings"
      ],
      "metadata": {
        "id": "lIvTPvcwnxCw"
      },
      "id": "lIvTPvcwnxCw"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "25a1eae6",
      "metadata": {
        "code_folding": [],
        "id": "25a1eae6"
      },
      "outputs": [],
      "source": [
        "# Create dictionary with embeddings\n",
        "def create_emb_dictionary(path):\n",
        "    # Crea un diccionario para almacenar las incrustaciones\n",
        "    emb_dict = {}\n",
        "\n",
        "    # abrimos la base de datos\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            parts = line.strip().split(' ')  # Supongamos que las incrustaciones están separadas por espacios.\n",
        "            word = parts[0]  # La primera parte es la palabra o elemento.\n",
        "            embedding = [float(val) for val in parts[1:]]  # El resto son los valores.\n",
        "            emb_dict[word] = embedding\n",
        "\n",
        "        # Retorna el diccionario creado\n",
        "    return emb_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the function to create the dictionary of the embeddings"
      ],
      "metadata": {
        "id": "pIcejOoUn1_3"
      },
      "id": "pIcejOoUn1_3"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "347a0e5e",
      "metadata": {
        "code_folding": [],
        "id": "347a0e5e",
        "outputId": "f718c255-c96e-47c0-9280-84a6c6ab7f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-83d6de182ae9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0membeddings_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_emb_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-66a919f0558a>\u001b[0m in \u001b[0;36mcreate_emb_dictionary\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# abrimos la base de datos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Supongamos que las incrustaciones están separadas por espacios.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B/glove.6B.200d.txt'"
          ]
        }
      ],
      "source": [
        "# create dictionary\n",
        "embeddings_dict = create_emb_dictionary(PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the file for further faster use"
      ],
      "metadata": {
        "id": "5z-PAvykn7XB"
      },
      "id": "5z-PAvykn7XB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f01b760",
      "metadata": {
        "code_folding": [
          1
        ],
        "id": "1f01b760"
      },
      "outputs": [],
      "source": [
        "# Serialize\n",
        "# Guarda el diccionario en un archivo serializado para cargarlo rápidamente en el futuro.\n",
        "with open('embeddings_dict_200D.pkl', 'wb') as output_file:\n",
        "  pickle.dump(emb_dict, output_file)\n",
        "\n",
        "# Deserialize\n",
        "# with open('embeddings_dict_200D.pkl', 'rb') as f:\n",
        "#     embeddings_dict = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0ab9aa",
      "metadata": {
        "id": "fd0ab9aa"
      },
      "source": [
        "#### See some embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b0991a9",
      "metadata": {
        "id": "8b0991a9"
      },
      "outputs": [],
      "source": [
        "# muestra los primeros embeddings\n",
        "def show_n_first_words(path, n_words):\n",
        "        with open(path, 'r') as f:   #abrimos el path\n",
        "            for i, line in enumerate(f):\n",
        "                print(line.split(), len(line.split()[1:]))\n",
        "                if i>=n_words: break  #aqui nos salimos del ciclo si ya llegamos al numero de n_words (embeddings a imprimir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a16259ec",
      "metadata": {
        "id": "a16259ec"
      },
      "outputs": [],
      "source": [
        "show_n_first_words(PATH, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff828123",
      "metadata": {
        "id": "ff828123"
      },
      "source": [
        "### Plot some embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3ffaa4",
      "metadata": {
        "id": "3d3ffaa4"
      },
      "outputs": [],
      "source": [
        "#\n",
        "def plot_embeddings(emb_path, words2show,emb_dim, embeddings_dict, func):\n",
        "    # Cargar el diccionario de incrustaciones desde el archivo serializado\n",
        "    with open(emb_path, 'rb') as input_file:      #emb_path tiene el serializado donde estan los embeddings\n",
        "        embeddings_dict = pickle.load(input_file)\n",
        "\n",
        "    # Obtener las palabras y las incrustaciones en una matriz NumPy\n",
        "    embeddings_matrix = np.array([embeddings_dict[word] for word in words2show])\n",
        "\n",
        "    if func == 'PCA':\n",
        "        # Aplicar PCA para reducir las dimensiones\n",
        "        pca = PCA(n_components=emb_dim)\n",
        "        embeddings = pca.fit_transform(embeddings_matrix)\n",
        "    elif func == 'TSNE':\n",
        "        # Aplicar t-SNE para reducir las dimensiones\n",
        "        tsne = TSNE(n_components=emb_dim)\n",
        "        embeddings = tsne.fit_transform(embeddings_matrix)\n",
        "    else: #ponemos una condicion para mostrar que no enviamos bien el parametro para reducir dimensiones\n",
        "        raise ValueError(\"Func debe ser 'PCA' o 'TSNE'\")\n",
        "\n",
        "    num_elements = len(words2show)    #obtemos el numero de elementos a mostrar\n",
        "\n",
        "    plt.figure(figsize=(13, 10))      #tamano de la figura (eje x, eje y)\n",
        "    sns.set()\n",
        "\n",
        "    x = embeddings[:num_elements, 0]  # Usa solo los primeros 'num_elements'\n",
        "    y = embeddings[:num_elements, 1]\n",
        "\n",
        "    # Crea un scatter plot\n",
        "    plt.scatter(x, y, alpha=0.5)\n",
        "\n",
        "    # Etiqueta cada punto con la palabra correspondiente\n",
        "    for i, word in enumerate(words2show):\n",
        "        plt.annotate(word, (x[i], y[i]), alpha=0.7)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following list is the one to graph"
      ],
      "metadata": {
        "id": "eCd4AfbnoUar"
      },
      "id": "eCd4AfbnoUar"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1b120a",
      "metadata": {
        "id": "9b1b120a"
      },
      "outputs": [],
      "source": [
        "#embeddings a mostrar en las graficas, se espera que se agrupen como estan en cada renglon\n",
        "words= ['burger', 'tortilla', 'bread', 'pizza', 'beef', 'steak', 'fries', 'chips',\n",
        "            'argentina', 'mexico', 'spain', 'usa', 'france', 'italy', 'greece', 'china',\n",
        "            'water', 'beer', 'tequila', 'wine', 'whisky', 'brandy', 'vodka', 'coffee', 'tea',\n",
        "            'apple', 'banana', 'orange', 'lemon', 'grapefruit', 'grape', 'strawberry', 'raspberry',\n",
        "            'school', 'work', 'university', 'highschool']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grapgh with PCA"
      ],
      "metadata": {
        "id": "Yl2oVwcBoaUB"
      },
      "id": "Yl2oVwcBoaUB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d336259a",
      "metadata": {
        "id": "d336259a"
      },
      "outputs": [],
      "source": [
        "#\n",
        "plot_embeddings(\"embeddings_dict_200D.pkl\", words, 2, embeddings_dict, \"PCA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graph with TSNE"
      ],
      "metadata": {
        "id": "YQO69g6socCo"
      },
      "id": "YQO69g6socCo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f22bacb4",
      "metadata": {
        "id": "f22bacb4"
      },
      "outputs": [],
      "source": [
        "# t-SNE dimensionality reduction for visualization\n",
        "#mandamos llamar la misma funcion de graficar, pero ahora con TSNE\n",
        "embeddings = plot_embeddings(\"embeddings_dict_200D.pkl\", words, 2, embeddings_dict, \"TSNE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8211b7f1",
      "metadata": {
        "id": "8211b7f1"
      },
      "source": [
        "### Let us compute analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analogy function, it creates the target vector by substracting the first 2 words and adding the third one, the minimum distance of a vector in the dictionary to this new vector would be the best analogy."
      ],
      "metadata": {
        "id": "IV8lYO9OoesB"
      },
      "id": "IV8lYO9OoesB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the minimum distance between a target vector and the subtraction of vectors is a way to perform word analogies in an embedding space. This is based on the concept that the direction in which the vector \"man\" must move to reach \"woman\" is the same direction in which the vector \"king\" must move to reach \"queen.\""
      ],
      "metadata": {
        "id": "WOHUBK_Lqk4i"
      },
      "id": "WOHUBK_Lqk4i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0003ce18",
      "metadata": {
        "id": "0003ce18"
      },
      "outputs": [],
      "source": [
        "# analogy\n",
        "#funcion para obtener la analogia\n",
        "def analogy(word1, word2, word3, embeddings_dict):\n",
        "    word1, word2, word3 = word1.lower(), word2.lower(), word3.lower()\n",
        "\n",
        "    # Obtén los vectores de incrustación para las palabras y conviértelos a vectores NumPy\n",
        "    vec_a, vec_b, vec_c = np.array(embeddings_dict[word1]), np.array(embeddings_dict[word2]), np.array(embeddings_dict[word3])\n",
        "\n",
        "    # Calcula el vector objetivo como: vec_b - vec_a + vec_c\n",
        "    vec_target = vec_b - vec_a + vec_c\n",
        "\n",
        "    # Encuentra la palabra más similar al vector objetivo\n",
        "    min_dist = float(\"inf\")  #creamos una variable inicializada con valor positivo infinito\n",
        "    best_word = None  #creamos una variable inicializada con \"null\"\n",
        "\n",
        "    for word, vec in embeddings_dict.items():     #iteramos en el diccionario, word para la clave y vec con el valor del vector\n",
        "        if word not in [word1, word2, word3]:     #checamos si la palabra del diccionario NO es word1, 2 o 3 pra calcular la distancia de ese vector al target\n",
        "            dist = np.linalg.norm(vec - vec_target) #guardamos la distancia\n",
        "            if dist < min_dist:                   #checamos si la distancia es la menor a alguna que tenemos\n",
        "                min_dist = dist                   #si es la menor, la guardamos.\n",
        "                best_word = word                  #almacenamos la mejor palabra en best_word\n",
        "\n",
        "    return best_word\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the analogy"
      ],
      "metadata": {
        "id": "Xzw_34kho3WI"
      },
      "id": "Xzw_34kho3WI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c02ec01",
      "metadata": {
        "id": "9c02ec01"
      },
      "outputs": [],
      "source": [
        "analogy('boy', 'son', 'girl', embeddings_dict)  #ponemos a prueba con nino es a hijo como nina es a... (esperemos que regrese hija: daughter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets fing the most similar words"
      ],
      "metadata": {
        "id": "hK6FPBUso5bf"
      },
      "id": "hK6FPBUso5bf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "cosine similarity is calculated using the formula for the dot product of two vectors divided by the product of their norms (lengths). Here is the formula for calculating cosine similarity between two vectors A and B:\n",
        "\n",
        "Cosine Similarity = (A · B) / (||A|| * ||B||)\n",
        "\n",
        "Where:\n",
        "\n",
        "(A · B) represents the dot product of vectors A and B.\n",
        "||A|| represents the norm (length) of vector A.\n",
        "||B|| represents the norm (length) of vector B"
      ],
      "metadata": {
        "id": "cIM9m07erBj9"
      },
      "id": "cIM9m07erBj9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb50e0f7",
      "metadata": {
        "id": "bb50e0f7"
      },
      "outputs": [],
      "source": [
        "def find_most_similar(word, embeddings_dict, top_n=10):\n",
        "    if word not in embeddings_dict:\n",
        "        return []  # La palabra no está en el diccionario de incrustaciones\n",
        "\n",
        "    # Obtén el vector de incrustación de la palabra de entrada\n",
        "    word_vector = embeddings_dict[word]\n",
        "\n",
        "    # Calcula la similitud coseno entre el vector de la palabra de entrada y todos los demás vectores\n",
        "    similarities = {}\n",
        "    for other_word, other_vector in embeddings_dict.items():        #iteramos en el diccionario, other_word para la clave y other_vector con el valor del vector\n",
        "        if other_word != word:                                      #si la palabra es diferente a la palabra que queremos encontrar la similiud, entonces calculamos la distancia\n",
        "            similarity = np.dot(word_vector, other_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(other_vector))\n",
        "            similarities[other_word] = similarity\n",
        "\n",
        "    # Ordena las palabras por similitud coseno y toma las \"top_n\" más similares\n",
        "    most_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return most_similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7946d712",
      "metadata": {
        "id": "7946d712"
      },
      "outputs": [],
      "source": [
        "most_similar = find_most_similar('book', embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b6e0ea1",
      "metadata": {
        "id": "3b6e0ea1"
      },
      "outputs": [],
      "source": [
        "for i, w in enumerate(most_similar, 1):\n",
        "    print(f'{i} ---> {w[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09875cae",
      "metadata": {
        "id": "09875cae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "985bfda4",
      "metadata": {
        "id": "985bfda4"
      },
      "outputs": [],
      "source": [
        "def find_most_similar_Bydist(word, embeddings_dict, top_n=10):\n",
        "    if word not in embeddings_dict:\n",
        "        return []  # La palabra no está en el diccionario de incrustaciones\n",
        "\n",
        "    # Obtén el vector de incrustación de la palabra de entrada\n",
        "    word_vector = embeddings_dict[word]\n",
        "\n",
        "    # Calcula la similitud coseno entre el vector de la palabra de entrada y todos los demás vectores\n",
        "    similarities = {}\n",
        "    for other_word, other_vector in embeddings_dict.items():        #iteramos en el diccionario, other_word para la clave y other_vector con el valor del vector\n",
        "        if other_word != word:                                      #si la palabra es diferente a la palabra que queremos encontrar la similiud, entonces calculamos la distancia\n",
        "\n",
        "            similarity = np.linalg.norm(vec - vec_target) #guardamos la distancia\n",
        "            similarities[other_word] = similarity\n",
        "\n",
        "    # Ordena las palabras por similitud coseno y toma las \"top_n\" más similares\n",
        "    most_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "\n",
        "    return most_similar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f06f33c5",
      "metadata": {
        "id": "f06f33c5"
      },
      "outputs": [],
      "source": [
        "most_similarbyDist = find_most_similar_Bydist('book', embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e86fa7",
      "metadata": {
        "id": "05e86fa7"
      },
      "outputs": [],
      "source": [
        "for i, w in enumerate(most_similarbyDist, 1):\n",
        "    print(f'{i} ---> {w[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1e1838b",
      "metadata": {
        "id": "e1e1838b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1a9fad",
      "metadata": {
        "id": "fe1a9fad"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}